---
title: "Practica - Analisis de Redes Sociales - MDSF"
author: "Claudia Quintana Wong"
output:
  html_document:
    df_print: paged
---

Este R Markdown recoge el enunciado de la práctica de la asignatura de redes sociales.

El objetivo es analizar un grafo, que se provee como fichero en el mismo paquete que este enunciado. En este fichero, encontramos solamente dos columnas, correspondiente a una interacción entre dos nodos de la red. Esta red está formada por distintos individuos que tienen contactos cara a cara durante un período de tiempo.

A continuación, dividimos la práctica en apartados, con una breve descripción de qué debe contener cada chunk de código donde el alumno desarrollará su respuesta así como las explicaciones que considere oportunas. Por favor, razona todas tus soluciones y escribe las explicaciones en azul.

Junto al título de cada apartado se encuentra la puntuación del mismo (pueden obtenerse hasta 10,5 puntos, aunque solamente se evaluará del 0 al 10).

## Carga de datos y comprobaciones iniciales (0,5 puntos)

En este apartado, se pide:

* Cargar el fichero adjunto en la práctica.
* Convertirlo en un objeto grafo de IGraph. Se cargará como un grafo NO dirigido.
* Comprobar que, efectivamente, tiene el número de nodos y enlaces correcto.
* Simplificar: eliminar bucles y agregar enlaces múltiples, contando cuántas veces aparece un enlace y almacenándolo como un peso de la red resultante.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(igraph)
library(ggplot2)
```

<span style="color: blue;"> Primeramente se carga el grafo en memoria y se comprueba la cantidad de aristas y nodos. Asimismo se calcula la densidad del grafo original. </span>

```{r}
dd <- read.csv("red_contactos.csv", sep=';')
g<-graph.data.frame(dd, directed=FALSE)
vcount(g)
ecount(g)
graph.density(g)
```

<span style="color: blue;">
En este apartado comprobamos que efectivamente el gráfico contiene ciclos. Para transformarlo creamos a una propiedad *weight* en cada una de las aristas y le asignamos valor 1. Luego, eliminamos los ciclos sumando la propiedad *weight* para asignar como peso la cantidad de aristas que existían. 
</span>

```{r}
sum(is.loop(g))

E(g)$weight <- 1

g <- simplify(g, edge.attr.comb = "sum")
sum(is.loop(g))

```
## Selección de la componente conexa mayor (0,5 puntos)

<span style="color: blue;">
En este punto, se pide realizar los pasos adecuados para generar un nuevo objeto grafo, que sea conexo, y que involucre a todos los nodos y enlaces de la componente conexa mayor del grafo original.

</span>

```{r}
is.connected(g)
components <- clusters(g, mode="weak")
biggest_cluster_id <- which.max(components$csize)

vert_ids <- V(g)[components$membership == biggest_cluster_id]
conn_g <- induced_subgraph(g, vids=vert_ids)
is.connected(conn_g)
```

## Análisis descriptivo de la componente conexa mayor (2,5 puntos)

En este apartado, se pide analizar descriptivamente el grafo usando los conceptos que hemos visto durante las clases de teoría:

* Grado medio
* Distancia media
* Diámetro
* Distribución de grados y ajuste a una Power-Law
* Clustering
* Entropía de los nodos
* Centralidad de los nodos y comparación con métricas de grado y clustering


<span style="color: blue;"> En el próximo fragmento de código se puede observar que el grafo tiene un gradio medio aproximado a 78, lo que implica, que cada nodo en media está conectado con 78 más. Asimismo, se puede notar que la distancia media es de aproximadamente 3 y el diámetro, 6.
</span>

```{r}
#grado medio
mean(degree(conn_g))

#distancia media
average.path.length(conn_g)

#diametro
diameter(conn_g)
```

<span style="color: blue;"> La distribución de los grados de la mayor componente conexa del grafo inicial se visualiza en un histograma, que muestra que gran parte de los nodos tienen grado menor que 150.</span>

```{r}
d = degree(conn_g, mode = "all")
hist(d)
```

<span style="color: blue;"> El siguiente gráfico muestra la distribución de los grados en función del logaritmo. </span>

```{r}
plot_degree_distribution = function(graph) {
    d = degree(graph, mode = "all")
    dd = degree.distribution(graph, mode = "all", cumulative = FALSE)
    degree = 1:max(d)
    probability = dd[-1]
    nonzero.position = which(probability != 0)
    probability = probability[nonzero.position]
    degree = degree[nonzero.position]
    plot(probability ~ degree, log = "xy", xlab = "Degree (log)", ylab = "Probability (log)", 
        col = 1, main = "Degree Distribution")
}

plot_degree_distribution(conn_g)
```

<span style="color: blue;"> En la siguiente gráfica se muestra el ajuste Power-Law teniendo en cuenta la distribución de grados antes vista.</span>

```{r}

fit_power_law = function(graph) {
    d = degree(graph, mode = "all")
    dd = degree.distribution(graph, mode = "all", cumulative = FALSE)
    degree = 1:max(d)
    probability = dd[-1]
    nonzero.position = which(probability != 0)
    probability = probability[nonzero.position]
    degree = degree[nonzero.position]
    reg = lm(log(probability) ~ log(degree))
    cozf = coef(reg)
    power.law.fit = function(x) exp(cozf[[1]] + cozf[[2]] * log(x))
    alpha = -cozf[[2]]
    R.square = summary(reg)$r.squared
    print(paste("Alpha =", round(alpha, 3)))
    print(paste("R square =", round(R.square, 3)))
    plot(probability ~ degree, log = "xy", xlab = "Degree (log)", ylab = "Probability (log)", 
        col = 1, main = "Degree Distribution")
    curve(power.law.fit, col = "red", add = T, n = length(d))
}

fit_power_law(conn_g)

```

<span style="color: blue;"> Un algoritmo de agrupamiento encuentra una asociación de los nodos a diferentes grupos según la cantidad de enlaces entre ellos. En este caso, se puede notar que la componente conexa mayor es fuertemente conexa, lo que quiere decir que existe un camino entre todo par de nodos. </span>

```{r}
# Clustering
components <- clusters(conn_g, mode="strong")
components$csize
components$no
```

<span style="color: blue;">La entropía de un grafo puede ser interpretada como la diversidad de un grafo, de manera que, permite tener una noción de la intensidad de los nodos en función de la cantidad de enlaces. Para resolver este punto se utiliza la función *diversity* que calcula una medida de diversidad para todos los vertices, de manera que la salida es un vector del tamaño de la cantidad de nodos de la red. </span>

```{r}
# entropy
diversit = diversity(conn_g)
```
<span style="color: blue;">La centralidad de un nodo se puede calcular de diversas manerasy puede ser interpretada como la importancia de un nodo en la red.. En este caso, se calcula como la cantidad de veces que un nodo aparece en el camino mínimo entre dos otros nodos. Esta métrica está relacionada con el grado de los nodos de manera, que mientras más grande es el grado medio de un grafo, más grande debiera ser la centralidad media. </span>

```{r}
#centrality
bets<-betweenness(conn_g,directed=FALSE)
mean(bets)
```


## Análisis de comunidades de la componente conexa mayor (1,5 puntos)

En este apartado, se pide aplicar dos algoritmos de detección de comunidades, compararlos y seleccionar cuál es, en tu opinión, el que da una mejor respuesta. Razona tu selección.

<span style="color: blue;"> El objetivo de encontrar comunidades es identificar los nodos que están más densamente conectados. Un criterio de evaluación para evaluar la salida de dos algoritmos de detección de comunidades es en función de la densidad intra/extra comunitaria. Se espera de una comunidad adecuada que la densidad intra-comunidad sea sea mayor que la externa, es decir, que tenga mayor interacción con los nodos que se encuentran dentro de la comunidad que con los de fuera.
</span>

<span style="color: blue;"> Sin embargo, si bien este criterio nos da una idea de la distancia entre nodos no nos dice nada sobre la estructura de la red. Por esta razón utilizaremos como métrica de comparación la modularidad, puesto que todos los algoritmos se centran en maximizarla, de manera que una valor de modularidad grande indica que la estructura es más clara y mejor la partición lograda.
</span>

<span style="color: blue;">
Los algoritmo seleccionados son en Greedy y el Multilevel. 

</span>
```{r}
c1<-fastgreedy.community(conn_g) #greedy
length(c1)
sizes(c1)
modularity(c1)
```

```{r}
c2 <- multilevel.community(conn_g)
length(c2)
modularity(c2)

```
<span style="color: blue;">
Al comparar ambos resultados se puede notar que la partición lograda aplicando Greedy tiene una mayor modularidad, por lo tanto, nos quedamos con esta detección. En ninguno de los dos casos se alcanza un valor de la modularidad alto, lo cual puede deberse a los algoritmos seleccionados o a la naturaleza intrínseca de los datos. También es notable que las métricas no son muy diferentes, lo cual sugiere que las particiones obtenidas por ambos algoritmos pueden ser similares. Comparamos las comunidades obtenidas utilizando la Información Mutua Normalizada.
</span>

```{r}
compare(comm1 = c1, comm2 = c2,method = "nmi")
```
<span style="color: blue;"> El valor obtenido muestra que se parecen en un 80%.</span>


## Visualización del grafo por comunidades de la componente conexa mayor (1,5 puntos)

En este apartado, se pide visualizar el grafo coloreando cada nodo en función de la comunidad a la que pertenezca, según tu elección del apartado anterior.

```{r}
### Inserta aqui tu codigo
cols<-rainbow(max(c1$membership))[c1$membership]

ll<-layout.fruchterman.reingold(conn_g)

png("c1.png",width=1200, height=1200, res=100)
plot(conn_g,layout=ll,vertex.label="",
     vertex.color=cols,
     vertex.size=log(bets+2))
#dev.off()
```

<span style="color: blue;"> Como se había mostrado cuando se analizó la partición greedy, este algoritmo ha identificado 6 comunidades. Sin embargo, se puede observar que la mayoría de los nodos se agrupan en dos grandes comunidades, representadas por el rojo y el amarillo.Esto sugiere que el algoritmo puede haber detectado comportamientos anómalos mientras buscaba las comunidades. 
</span>

## Difundiendo un rumor (o un virus) en la componente conexa mayor (4 puntos)

Este apartado es el que más peso en la práctica tiene. Vamos a implementar un modelo epidemiológico sobre el grafo que, típicamente, se utiliza para simular escenarios de difusión de enfermedades pero también en contextos como la distribución de rumores e información. Vamos a implementar un modelo SIR que se caracteriza por tener los siguientes parámetros:

* Número de nodos iniciales infectados en el momento t=0 (N).
* Beta: probabilidad de contagio de un nodo infectado (I) a un nodo susceptible de serlo (S)
* Gamma: probabilidad de que un nodo infectado (I) se recupere en momenteo actual (R). Los nodos en estado (R) no son susceptibles y permanecen en este estado infinitamente.

Se pide desarrollar una función que tenga como parámetros los tres valores anteriores y un cuarto que sea un grafo que, en nuestro caso, será la componente conexa mayor del grafo original de esta práctica. Dicha función simulará el proceso SIR:

* En t=0, se seleccionan N nodos al azar, que pasarán a estado infectado.
* En t=1, se podrán contagiar con probabilidad Beta nodos que tienen un vecino infectado; OJO: si un nodo en estado S tiene varios vecinos en estado I tiene más probabilidad de infectarse ya que cada vecino tendrá un intento de infectarle.
* Se repite el paso anterior sucesivamente, hasta que no vemos infectados nuevos durante, al menos, 3 iteraciones.

Se pide ejecutar una simulación para tres o cuatro valores del parámetro beta (N y gamma pueden ser fijos en estas simulaciones) de este proceso de manera que se pueda visualizar:

* La curva de nuevos infectados en escala logarítmica para cada caso.
* El grafo que surge de la cascada de contagios: es decir, dos nodos están enlazados ahora si uno ha contagiado al otro. Como es lógico, tanto los nodos como los enlaces de este nuevo grafo son un subconjunto del grafo original.
 
<span style="color:blue"> El modelo SIR estima el número de individuos susceptibles a infectarse (S), el número de individuos infectados capaces de infectar (I) y el número de individuos recuperados (que se curaron o fallecieron) (R), donde *beta* puede ser interpretado como la probabilidad de transmisión y *gamma* como la probabilidad de recuperación.</span>

<span style="color:blue"> Para el diseño del algoritmo se tuvieron en cuenta las siguientes consideraciones:</span>

* <span style="color:blue">Que un nodo esté directamente conectado a través de una arista con otro nodo se interpreta en este contexto como un "contacto estrecho".</span>
* <span style="color:blue">Un nodo susceptibe puede convertirse en infectado en un momento t+1, si tiene al menos un vecino que está infectado en el momento *t*.</span>
* <span style="color:blue">Un nodo en el tiempo t tiene la posibilidad de recuperarse o infectar a otros. De manera que, si un nodo se recupera en un tiempo t no puede contagiar a ningún otro.</span>
</span>

<span style="color:blue">A continuación se explica detalladamente el algoritmo diseñado para simular la difusión de una epidemia</span>

1. <span style="color:blue">Inicialmente, se agrega una propiedad **state** a cada nodo del grafo que indica el estado actual y que puede tomar valores {S, I, R} para denotar que el nodo está Infectado, Susceptible o Recuperado.</span>
2. <span style="color:blue">Al inicio todos los nodos son suceptibles, no hay nadie infectado. Para simular el inicio de la epidemia se seleccionan N nodos aleatorios cuyo estado pasa a ser Infectado.</span>
3. <span style="color:blue">Durante la ejecución se almacenan en las variables *infected_hist*, *susceptible_hist* y *recovered_hist* el histórico de casos infectados, susceptibles y recuperados respectivamente.</span>
4. <span style="color:blue">En cada tiempo *t*, se itera por todos los nodos infectados. Una vez que estamos analizando un nodo infectado en el momento *t* pueden ocurrir tres cosas: que se recupere, que infecte a algunos de sus vecinos susceptibles o que no contagie a nadie porque no esté conectado con ningún susceptible.</span>
5. <span style="color:blue">Para simular una recuperación de un nodo se genera una probabilidad *prob_rec* aleatoria además del parámetro gamma. De manera que, un nodo infectado se recupera si $prob\_rec * gamma$ es mayor que un *threshold*. La introducción de esta aleatoriedad permite que no todos los nodos tengan la misma capacidad de recuperación, incluso, un mismo nodo, según el momento en que se encuentre puede tener una probabilidad u otra.</span>
6. <span style="color:blue">Por otra parte, para simular la posibilidad de infección se genera una probabilidad *prob_infect* para cada uno de los vecinos de un nodo *n*. De manera que, un nodo susceptible se contagia si la multiplicación de su *prob_infect* y beta es mayor que 0.5. En un escenario pandémico real, esta probabilidad generada se puede interpretar como el nivel de contacto con un infectado, puesto que no todos los contactos (en este caso, vecinos) tienen por qué tener el mismo tipo de contacto.</span>

<span style="color:blue">Con el fin de visualizar la red resultante de los contagios se crea un grafo vacío cuyos nodos se corresponden con los del grafo que estamo analizando y cada vex que un nodo se contagia se introduce una arista.</span>


```{r}

difusion_simulator <- function(N, beta, gamma, g)
{
  new_g <- make_empty_graph(n=length(V(g)), directed = FALSE)
  V(new_g)$name <- V(g)$name
  
  V(g)$state <- 'S'
  
  # t=0
  infected_index <- sample.int(length(V(g)), N)
  V(g)[infected_index]$state <- 'I'
  
  infected_nodes <- V(g)[infected_index]
  
  infected_hist <- c(N)
  susceptible_hist <- c(length(V(g)))
  recovered_hist <- c(0)
  time_hist <- c(0)
  
  t = 1
  times <- 0
  while(times < 3)
  {
    time_hist <- c(time_hist, t)
    print(paste0("t = ", t))
    t = t + 1
    
    flag <- FALSE
    for (index in infected_nodes) 
    {
      node = V(g)[index]
      # print(paste0("Analizando nodo ", node$name))
      prob_rec <- runif(1) * gamma
      if (prob_rec >= 0.5) ## si un nodo en el tiempo t se recupera, no tiene sentido que infecte
      {
        # print(paste0('Recuperado nodo ', node$name))
        flag <- TRUE
        V(g)[index]$state <-'R'
        
      }
      else
      {
        neig <- neighbors(g, node)  # los vecinos del nodo infectado
        susc_neighbors <- neig[neig$state == 'S']
        n <- length(susc_neighbors)
        n_prob <- runif(n) # genera un vector de tamaño n (vecinos susceptibles)
        if (n > 0)
        {
          for (i in c(1:n))
          {
            susc_node = susc_neighbors[i]
            prob_infect <- beta * n_prob[i]
            
            if (prob_infect >= 0.3)
            {
              # print(paste0("Infectado nodo ", susc_node$name))
              V(g)[V(g)$name == susc_node$name]$state = 'I'
              flag <- TRUE
              new_g <- new_g + edge(node$name, susc_node$name, color = "red")
            }
          }
        }
      }
    }
    if (flag == FALSE) #no hubo cambios en t, aumentamos el contador
    {
      times = times + 1
      
    }
    else # hubo cambios en el tiempo t
    {
        times = 0
        flag= FALSE
    }
    
    infected_nodes = V(g)[V(g)$state == 'I']
    
    infected_hist <- c(infected_hist, length(infected_nodes))
    susceptible_hist <- c(susceptible_hist, length(V(g)[V(g)$state == 'S']))
    recovered_hist <- c(recovered_hist, length(V(g)[V(g)$state == 'R']))
  }
  
  df <- data.frame(infected_hist, susceptible_hist, recovered_hist, time_hist)
  result <-list("df"= df, "graph" = g, "new_graph"=new_g)
  return (result)
}

```

<span style="color:blue">A continuación se ejecuta el algoritmo variando la cantidad de nodos contagiados inicialmente. En cada caso, se ejecuta el algoritmo y se presentan el gráfico de contagios en escala logarítmica y el grafo resultante cuyas aristas representan que un nodo x contagió a un nodo y. Se ejecuta el algoritmo con N = 10</span>

```{r}
r1 = difusion_simulator(10, 0.6, 0.5, conn_g)

```
```{r}
df1 <- r1$df
g1 <- r1$graph
ng1 <- r1$new_graph

ggplot(df1, aes(x=time_hist, y=log(infected_hist))) +
  geom_line() +
  geom_point()
```


```{r}
plot(ng1, vertex.size=10, vertex.label=NA)
```

<span style="color:blue">N = 50</span>

```{r}
r2 = difusion_simulator(50, 0.5, 0.6, conn_g)
```
```{r}

df2 <- r2$df
g2 <- r2$graph
ng2 <- r2$new_graph

ggplot(df2, aes(x=time_hist, y=log(infected_hist))) +
  geom_line() +
  geom_point()
```


```{r}
plot(ng2, vertex.size=10, vertex.label=NA)
```

<span style="color:blue">N = 100</span>

```{r}
r3 = difusion_simulator(100, 0.4, 0.7, conn_g)
```
```{r}
df3 <- r3$df
g3 <- r3$graph
ng3 <- r3$new_graph

ggplot(df3, aes(x=time_hist, y=log(infected_hist))) +
  geom_line() +
  geom_point()
```

```{r}
plot(ng3, vertex.size=10, vertex.label=NA)
```


<span style="color:blue"> En este ejercicio se ha implementado un simulador de epidemias. El agoritmo propuesto, y que no refleja el comportamiento real de una epidemia, esta muy influenciado por la aleatoriedad por lo que la elección de los parámetros, tanto el beta, el gamma como los threshold propios del algoritmo influyen en gran medida en los resultados alcanzados. Los parámetros que aquí se muestran se han seleccionado después de varios experimentos teniendo en cuenta que con ellos el algoritmo es capaz de mostrar las diferentes casuísticas y propiciando que converjan en un pequeño número de pasos para reeducir el tiempo de ejecución. De manera general, se puede afirmar que mientras mas grande el número de contagiados inicial, más demora el algoritmo en converger, atendiendo también a las probabilidades. Asimismo, se puede notar que mientras más grande el parámetro beta más infectados (caso 1) y mientras más grande el gamma, la curva de contagios muestra un descenso rápido (caso 3). </span>





